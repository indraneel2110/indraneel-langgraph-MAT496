{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9e547f",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239464-lesson-1-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "## Review\n",
    "\n",
    "In module 2, covered a few ways to customize graph state and memory.\n",
    " \n",
    "We built up to a Chatbot with external memory that can sustain long-running conversations. \n",
    "\n",
    "## Goals\n",
    "\n",
    "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways. \n",
    "\n",
    "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b430d92-f595-4322-a56e-06de7485daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0682fc",
   "metadata": {},
   "source": [
    "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0) \n",
    "\n",
    "# State \n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State, config: RunnableConfig):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages, config)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State):\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f847a787-b301-488c-9b58-cba9f389f55d",
   "metadata": {},
   "source": [
    "### Streaming full state\n",
    "\n",
    "Now, let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "`.stream` and `.astream` are sync and async methods for streaming back results. \n",
    " \n",
    "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
    " \n",
    "* `values`: This streams the full state of the graph after each node is called.\n",
    "* `updates`: This streams updates to the state of the graph after each node is called.\n",
    "\n",
    "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
    "\n",
    "Let's look at `stream_mode=\"updates\"`.\n",
    "\n",
    "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
    "\n",
    "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation': {'messages': AIMessage(content=\"Hi Kamal! I can't provide real-time weather updates. For the current weather in Gurgaon, I recommend checking a reliable weather website or app such as Weather.com, AccuWeather, or using a weather service on your smartphone.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 296, 'total_tokens': 342, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUbZdbU1RzqiGbTL2MeJJlNKDNkXY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--b255cde1-5d63-4160-9b5e-678e549a15b5-0', usage_metadata={'input_tokens': 296, 'output_tokens': 46, 'total_tokens': 342, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n",
      "{'summarize_conversation': {'summary': \"The conversation began with Lance introducing himself, followed by a brief greeting exchange. Later, Indraneel introduced himself and asked about the staple food in Goa. The response highlighted that rice and fish curry are staple foods in Goa, with seafood being a significant part of the diet due to the coastal location. The summary also mentioned the influence of Indian and Portuguese cuisines, with common ingredients like coconut and spices.\\n\\nSubsequently, Kamal introduced himself as a friend of Indraneel's and inquired about the current weather in Gurgaon. The response indicated that real-time weather updates are not available, and suggested using a weather website or app like Weather.com or AccuWeather for accurate information. Kamal reiterated his introduction, and the same guidance regarding checking the weather through reliable sources was provided.\", 'messages': [RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='52382a9d-2e07-4dfc-af0f-b43789f0cf64'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='lc_run--7a419001-705c-4c51-9a85-d78c3501b80c-0'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='7e987ca5-43e1-4eed-a746-8955a66a0083'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='90641ff3-672b-4d4e-ad57-a09cd793e4a7'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='lc_run--5e897d2a-955f-4025-afc7-72c04143de5a-0'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='80e0dfd0-b556-4752-bae6-461f76ac3877')]}}\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Kamal, a friend of Indraneel's\"), HumanMessage(content=\"What is the weather in Gurgaon right now\")]}, config, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
   "metadata": {},
   "source": [
    "Let's now just print the state update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c859c777-cb12-4682-9108-6b367e597b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Siddhu! How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Siddhu\")]}, config, stream_mode=\"updates\"):\n",
    "    chunk['conversation'][\"messages\"].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583bf219-6358-4d06-ae99-c40f43569fda",
   "metadata": {},
   "source": [
    "Now, we can see `stream_mode=\"values\"`.\n",
    "\n",
    "This is the `full state` of the graph after the `conversation` node is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Lance\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Lance! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Siddhu\n",
      "---------------------------------------------------------------------------\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Lance\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Lance! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm Siddhu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Siddhu! How can I help you today?\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Siddhu\")\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    for m in event['messages']:\n",
    "        m.pretty_print()\n",
    "    print(\"---\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
   "metadata": {},
   "source": [
    "### Streaming tokens\n",
    "\n",
    "We often want to stream more than graph state.\n",
    "\n",
    "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
    "\n",
    "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
    "\n",
    "Each event is a dict with a few keys:\n",
    " \n",
    "* `event`: This is the type of event that is being emitted. \n",
    "* `name`: This is the name of event.\n",
    "* `data`: This is the data associated with the event.\n",
    "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: . Type: on_chain_start. Name: LangGraph\n",
      "Node: conversation. Type: on_chain_start. Name: conversation\n",
      "Node: conversation. Type: on_chat_model_start. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chat_model_end. Name: ChatOpenAI\n",
      "Node: conversation. Type: on_chain_start. Name: should_continue\n",
      "Node: conversation. Type: on_chain_end. Name: should_continue\n",
      "Node: conversation. Type: on_chain_stream. Name: conversation\n",
      "Node: conversation. Type: on_chain_end. Name: conversation\n",
      "Node: . Type: on_chain_stream. Name: LangGraph\n",
      "Node: . Type: on_chain_end. Name: LangGraph\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
   "metadata": {},
   "source": [
    "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
    "\n",
    "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
    "\n",
    "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' stor', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ied', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' franchise', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' National', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' League', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='NFL', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='),', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' for', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' rich', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' history', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' success', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' on', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' field', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=\" Here's\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' an', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' overview', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Overview', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Location', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' based', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Bay', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Area', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' home', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' games', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' played', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Levi', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Stadium', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Santa', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Clara', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' California', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' since', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='201', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='4', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Found', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ing', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' were', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' established', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='194', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='6', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' part', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' All', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-Amer', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ica', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Conference', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='AA', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' joined', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='194', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' when', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' leagues', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' merged', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Colors', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Masc', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ot', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=\" team's\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' colors', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' red', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' gold', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' white', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' mascot', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' S', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ourd', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ough', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Sam', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Ach', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ievements', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Championships', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' five', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' titles', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' XVI', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' XIX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='III', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='IV', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='IX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='),', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' primarily', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' during', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' dominant', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' run', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='198', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' early', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='199', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Conference', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Division', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Titles', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' They', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' numerous', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' NFC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' championships', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' NFC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' West', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' division', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' titles', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' establishing', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' themselves', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' powerhouse', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' league', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Not', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='able', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Figures', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Players', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' been', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' home', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' several', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Hall', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Fame', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' players', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' including', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' -', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Joe', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Montana', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Legendary', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' quarterback', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' for', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' his', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' po', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ise', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' success', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' postseason', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' -', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Jerry', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Rice', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Wid', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ely', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' regarded', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' greatest', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' wide', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' receiver', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' history', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' -', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Steve', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Young', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Dual', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-threat', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' quarterback', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' who', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' succeeded', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Montana', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' continued', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=\" team's\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' success', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' -', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Ron', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='nie', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' L', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ott', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Hard', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-h', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='itting', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' safety', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' for', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' his', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' leadership', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' defensive', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' prowess', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Co', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='aches', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' \\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' -', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Bill', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Walsh', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Innov', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ator', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' West', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Coast', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' offense', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' leading', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' three', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' victories', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' -', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='George', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Se', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='if', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ert', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Continued', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=\" team's\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' success', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' two', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' more', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' wins', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Rival', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ries', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Dallas', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Cowboys', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' A', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' historic', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' rivalry', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' especially', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' intense', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' during', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='198', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='199', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' several', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' playoff', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' match', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ups', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Los', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Angeles', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Rams', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' A', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' divis', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ional', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' rivalry', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' that', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' been', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' competitive', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' fierce', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' over', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' years', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='Seattle', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Seahawks', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' A', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' more', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' recent', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' rivalry', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' that', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' grown', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' due', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' both', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' teams', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=\"'\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' competitiveness', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' NFC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' West', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Recent', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Performance', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='In', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' recent', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' years', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' been', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' competitive', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' reaching', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='201', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' season', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' where', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' were', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' narrowly', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' defeated', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' by', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Kansas', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' City', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Chiefs', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Under', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' head', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' coach', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Kyle', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' Shan', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ahan', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' been', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' for', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' its', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' strong', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' defense', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' innovative', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' offensive', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' schemes', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' continue', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' be', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' prominent', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' passionate', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' fan', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' base', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' commitment', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' building', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' on', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' historic', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content=' legacy', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3')}\n",
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3', chunk_position='last')}\n",
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3', usage_metadata={'input_tokens': 494, 'output_tokens': 554, 'total_tokens': 1048, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}\n",
      "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--de709db3-d759-4019-b74c-e92c16b3ffa3', chunk_position='last')}\n"
     ]
    }
   ],
   "source": [
    "node_to_stream = 'conversation'\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        print(event[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
   "metadata": {},
   "source": [
    "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| San| Francisco| |49|ers| are| a| stor|ied| franchise| in| the| National| Football| League| (|NFL|),| known| for| their| rich| history| and| success| on| the| field|.| Here| are| some| key| details| about| the| team|:\n",
      "\n",
      "|###| Overview|\n",
      "|-| **|Location|**|:| The| team| is| based| in| the| San| Francisco| Bay| Area|,| with| their| home| games| played| at| Levi|'s| Stadium| in| Santa| Clara|,| California|.\n",
      "|-| **|Founded|**|:| The| |49|ers| were| established| in| |194|6| as| part| of| the| All|-Amer|ica| Football| Conference| (|AA|FC|)| and| joined| the| NFL| in| |194|9| when| the| leagues| merged|.\n",
      "|-| **|Team| Colors|**|:| Red|,| gold|,| and| white|.\n",
      "\n",
      "|###| Ach|ievements|\n",
      "|-| **|Super| Bowl| Titles|**|:| The| |49|ers| have| won| five| Super| Bowl| championships| (|X|VI|,| XIX|,| XX|III|,| XX|IV|,| and| XX|IX|),| primarily| during| their| dominant| run| in| the| |198|0|s| and| early| |199|0|s|.\n",
      "|-| **|Conference| Championships|**|:| They| have| won| multiple| NFC| championships|,| reflecting| their| consistent| competitiveness| in| the| league|.\n",
      "|-| **|Division| Titles|**|:| The| team| has| numerous| NFC| West| division| titles|.\n",
      "\n",
      "|###| Not|able| Figures|\n",
      "|-| **|Players|**|:| The| |49|ers| have| had| several| Hall| of| Fame| players|,| including| quarterback| Joe| Montana|,| wide| receiver| Jerry| Rice|,| quarterback| Steve| Young|,| safety| Ronnie| L|ott|,| and| defensive| end| Charles| Haley|.\n",
      "|-| **|Co|aches|**|:| Bill| Walsh|,| the| legendary| head| coach|,| is| credited| with| developing| the| West| Coast| offense|,| which| became| a| hallmark| of| the| team's| success|.| George| Se|if|ert| also| led| the| team| to| multiple| Super| Bowl| victories|.\n",
      "|-| **|Current| Leadership|**|:| As| of| the| latest| updates|,| Kyle| Shan|ahan| serves| as| the| head| coach|,| and| John| Lynch| is| the| general| manager|.\n",
      "\n",
      "|###| Rival|ries|\n",
      "|-| **|Seattle| Seahawks|**|:| A| fierce| divis|ional| rivalry| that| has| grown| in| intensity| over| the| years|.\n",
      "|-| **|Los| Angeles| Rams|**|:| Another| key| divis|ional| rivalry|,| with| both| teams| having| a| long| history| of| competitive| match|ups|.\n",
      "|-| **|Dallas| Cowboys| and| Green| Bay| Packers|**|:| Historical| rival|ries|,| especially| prominent| during| the| |198|0|s| and| |199|0|s|.\n",
      "\n",
      "|###| Recent| Performance|\n",
      "|In| recent| years|,| the| |49|ers| have| been| competitive|,| highlighted| by| their| appearance| in| Super| Bowl| LIV| following| the| |201|9| season|,| where| they| were| narrowly| defeated| by| the| Kansas| City| Chiefs|.| The| team| is| known| for| its| strong| defense| and| innovative| offensive| strategies| under| Kyle| Shan|ahan|.\n",
      "\n",
      "|###| Ownership|\n",
      "|The| team| is| owned| by| the| York| family|,| with| Jed| York| serving| as| the| CEO|.| The| ownership| has| been| committed| to| maintaining| the| team's| competitive| edge| and| ensuring| its| success| both| on| and| off| the| field|.\n",
      "\n",
      "|The| San| Francisco| |49|ers| remain| one| of| the| most| iconic| and| successful| franchises| in| NFL| history|,| with| a| passionate| fan| base| and| a| legacy| of| excellence|.||||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        data = event[\"data\"]\n",
    "        print(data[\"chunk\"].content, end=\"|\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
   "metadata": {},
   "source": [
    "### Streaming with LangGraph API\n",
    "\n",
    "** DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "-  API: http://127.0.0.1:2024\n",
    "-  Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "-  API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.\n",
    "\n",
    "The LangGraph API [supports editing graph state](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/#initial-invocation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "079c2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "# This is the URL of the local development server\n",
    "URL = \"http://127.0.0.1:2024\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32",
   "metadata": {},
   "source": [
    "Let's [stream `values`](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/), like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamPart(event='metadata', data={'run_id': '019a1c89-5d57-70d2-884d-ddc1f7d6c306', 'attempt': 1})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Calculate the factorial of 10', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3ef250a8-4689-4853-9078-6b5f1734c5e2'}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Calculate the factorial of 10', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3ef250a8-4689-4853-9078-6b5f1734c5e2'}, {'content': \"The factorial of a number n, denoted as n!, is the product of all positive integers less than or equal to n.\\n\\nTo calculate 10!, it would be:\\n\\n10! = 10  9  8  7  6  5  4  3  2  1\\n\\nI'll calculate this by performing the necessary multiplications.\", 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 163, 'prompt_tokens': 134, 'total_tokens': 297, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUct3xLCwVOvNWvrTWDzsTNajlvDv', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--11ff5e1e-c526-4a66-8e09-e0df1c889e07-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 10, 'b': 9}, 'id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 8, 'b': 7}, 'id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 6, 'b': 5}, 'id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 4, 'b': 3}, 'id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 163, 'total_tokens': 297, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Calculate the factorial of 10', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3ef250a8-4689-4853-9078-6b5f1734c5e2'}, {'content': \"The factorial of a number n, denoted as n!, is the product of all positive integers less than or equal to n.\\n\\nTo calculate 10!, it would be:\\n\\n10! = 10  9  8  7  6  5  4  3  2  1\\n\\nI'll calculate this by performing the necessary multiplications.\", 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 163, 'prompt_tokens': 134, 'total_tokens': 297, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUct3xLCwVOvNWvrTWDzsTNajlvDv', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--11ff5e1e-c526-4a66-8e09-e0df1c889e07-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 10, 'b': 9}, 'id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 8, 'b': 7}, 'id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 6, 'b': 5}, 'id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 4, 'b': 3}, 'id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 163, 'total_tokens': 297, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '90', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'e2c607a7-dc17-4b87-a8e5-a0aad4e4284c', 'tool_call_id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'artifact': None, 'status': 'success'}, {'content': '56', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'a25bde41-d95d-4444-bfb4-86cbe344105a', 'tool_call_id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'artifact': None, 'status': 'success'}, {'content': '30', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '06b10658-dcd9-443f-b2ca-2243b10fd71c', 'tool_call_id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'artifact': None, 'status': 'success'}, {'content': '12', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '086bdada-641a-45cf-9b79-7ee646041082', 'tool_call_id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'artifact': None, 'status': 'success'}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Calculate the factorial of 10', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3ef250a8-4689-4853-9078-6b5f1734c5e2'}, {'content': \"The factorial of a number n, denoted as n!, is the product of all positive integers less than or equal to n.\\n\\nTo calculate 10!, it would be:\\n\\n10! = 10  9  8  7  6  5  4  3  2  1\\n\\nI'll calculate this by performing the necessary multiplications.\", 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 163, 'prompt_tokens': 134, 'total_tokens': 297, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUct3xLCwVOvNWvrTWDzsTNajlvDv', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--11ff5e1e-c526-4a66-8e09-e0df1c889e07-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 10, 'b': 9}, 'id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 8, 'b': 7}, 'id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 6, 'b': 5}, 'id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 4, 'b': 3}, 'id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 163, 'total_tokens': 297, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '90', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'e2c607a7-dc17-4b87-a8e5-a0aad4e4284c', 'tool_call_id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'artifact': None, 'status': 'success'}, {'content': '56', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'a25bde41-d95d-4444-bfb4-86cbe344105a', 'tool_call_id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'artifact': None, 'status': 'success'}, {'content': '30', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '06b10658-dcd9-443f-b2ca-2243b10fd71c', 'tool_call_id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'artifact': None, 'status': 'success'}, {'content': '12', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '086bdada-641a-45cf-9b79-7ee646041082', 'tool_call_id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'artifact': None, 'status': 'success'}, {'content': '', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 50, 'prompt_tokens': 322, 'total_tokens': 372, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUct8NnPIaDO4Zt5IbDDHul7Xfumg', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--e18f59c1-0ee0-4d9b-b212-2e8dae6ff057-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 90, 'b': 56}, 'id': 'call_LxVltuhGGxtdualNx9CydD2I', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 30, 'b': 12}, 'id': 'call_eiOdqgzGGFmiW2om5T7Xj5Nh', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 322, 'output_tokens': 50, 'total_tokens': 372, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Calculate the factorial of 10', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3ef250a8-4689-4853-9078-6b5f1734c5e2'}, {'content': \"The factorial of a number n, denoted as n!, is the product of all positive integers less than or equal to n.\\n\\nTo calculate 10!, it would be:\\n\\n10! = 10  9  8  7  6  5  4  3  2  1\\n\\nI'll calculate this by performing the necessary multiplications.\", 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 163, 'prompt_tokens': 134, 'total_tokens': 297, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUct3xLCwVOvNWvrTWDzsTNajlvDv', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--11ff5e1e-c526-4a66-8e09-e0df1c889e07-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 10, 'b': 9}, 'id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 8, 'b': 7}, 'id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 6, 'b': 5}, 'id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 4, 'b': 3}, 'id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 163, 'total_tokens': 297, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '90', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'e2c607a7-dc17-4b87-a8e5-a0aad4e4284c', 'tool_call_id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'artifact': None, 'status': 'success'}, {'content': '56', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'a25bde41-d95d-4444-bfb4-86cbe344105a', 'tool_call_id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'artifact': None, 'status': 'success'}, {'content': '30', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '06b10658-dcd9-443f-b2ca-2243b10fd71c', 'tool_call_id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'artifact': None, 'status': 'success'}, {'content': '12', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '086bdada-641a-45cf-9b79-7ee646041082', 'tool_call_id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'artifact': None, 'status': 'success'}, {'content': '', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 50, 'prompt_tokens': 322, 'total_tokens': 372, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUct8NnPIaDO4Zt5IbDDHul7Xfumg', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--e18f59c1-0ee0-4d9b-b212-2e8dae6ff057-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 90, 'b': 56}, 'id': 'call_LxVltuhGGxtdualNx9CydD2I', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 30, 'b': 12}, 'id': 'call_eiOdqgzGGFmiW2om5T7Xj5Nh', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 322, 'output_tokens': 50, 'total_tokens': 372, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '5040', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'ea77ac7a-e854-4fde-88a6-eac70cdb4dc2', 'tool_call_id': 'call_LxVltuhGGxtdualNx9CydD2I', 'artifact': None, 'status': 'success'}, {'content': '360', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '97570185-077e-4dd2-921d-217464f41a2b', 'tool_call_id': 'call_eiOdqgzGGFmiW2om5T7Xj5Nh', 'artifact': None, 'status': 'success'}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Calculate the factorial of 10', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3ef250a8-4689-4853-9078-6b5f1734c5e2'}, {'content': \"The factorial of a number n, denoted as n!, is the product of all positive integers less than or equal to n.\\n\\nTo calculate 10!, it would be:\\n\\n10! = 10  9  8  7  6  5  4  3  2  1\\n\\nI'll calculate this by performing the necessary multiplications.\", 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 163, 'prompt_tokens': 134, 'total_tokens': 297, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUct3xLCwVOvNWvrTWDzsTNajlvDv', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--11ff5e1e-c526-4a66-8e09-e0df1c889e07-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 10, 'b': 9}, 'id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 8, 'b': 7}, 'id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 6, 'b': 5}, 'id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 4, 'b': 3}, 'id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 163, 'total_tokens': 297, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '90', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'e2c607a7-dc17-4b87-a8e5-a0aad4e4284c', 'tool_call_id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'artifact': None, 'status': 'success'}, {'content': '56', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'a25bde41-d95d-4444-bfb4-86cbe344105a', 'tool_call_id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'artifact': None, 'status': 'success'}, {'content': '30', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '06b10658-dcd9-443f-b2ca-2243b10fd71c', 'tool_call_id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'artifact': None, 'status': 'success'}, {'content': '12', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '086bdada-641a-45cf-9b79-7ee646041082', 'tool_call_id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'artifact': None, 'status': 'success'}, {'content': '', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 50, 'prompt_tokens': 322, 'total_tokens': 372, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUct8NnPIaDO4Zt5IbDDHul7Xfumg', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--e18f59c1-0ee0-4d9b-b212-2e8dae6ff057-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 90, 'b': 56}, 'id': 'call_LxVltuhGGxtdualNx9CydD2I', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 30, 'b': 12}, 'id': 'call_eiOdqgzGGFmiW2om5T7Xj5Nh', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 322, 'output_tokens': 50, 'total_tokens': 372, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '5040', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'ea77ac7a-e854-4fde-88a6-eac70cdb4dc2', 'tool_call_id': 'call_LxVltuhGGxtdualNx9CydD2I', 'artifact': None, 'status': 'success'}, {'content': '360', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '97570185-077e-4dd2-921d-217464f41a2b', 'tool_call_id': 'call_eiOdqgzGGFmiW2om5T7Xj5Nh', 'artifact': None, 'status': 'success'}, {'content': '', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 18, 'prompt_tokens': 389, 'total_tokens': 407, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUctBB7yA2YYUaAv4AKBwQ6aBl3PU', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--d1975e17-6c8e-4640-a101-a9781321a459-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 5040, 'b': 360}, 'id': 'call_VJQ8PqXeJLLGeM5PZjp58i51', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 389, 'output_tokens': 18, 'total_tokens': 407, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Calculate the factorial of 10', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3ef250a8-4689-4853-9078-6b5f1734c5e2'}, {'content': \"The factorial of a number n, denoted as n!, is the product of all positive integers less than or equal to n.\\n\\nTo calculate 10!, it would be:\\n\\n10! = 10  9  8  7  6  5  4  3  2  1\\n\\nI'll calculate this by performing the necessary multiplications.\", 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 163, 'prompt_tokens': 134, 'total_tokens': 297, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUct3xLCwVOvNWvrTWDzsTNajlvDv', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--11ff5e1e-c526-4a66-8e09-e0df1c889e07-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 10, 'b': 9}, 'id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 8, 'b': 7}, 'id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 6, 'b': 5}, 'id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 4, 'b': 3}, 'id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 163, 'total_tokens': 297, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '90', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'e2c607a7-dc17-4b87-a8e5-a0aad4e4284c', 'tool_call_id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'artifact': None, 'status': 'success'}, {'content': '56', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'a25bde41-d95d-4444-bfb4-86cbe344105a', 'tool_call_id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'artifact': None, 'status': 'success'}, {'content': '30', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '06b10658-dcd9-443f-b2ca-2243b10fd71c', 'tool_call_id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'artifact': None, 'status': 'success'}, {'content': '12', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '086bdada-641a-45cf-9b79-7ee646041082', 'tool_call_id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'artifact': None, 'status': 'success'}, {'content': '', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 50, 'prompt_tokens': 322, 'total_tokens': 372, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUct8NnPIaDO4Zt5IbDDHul7Xfumg', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--e18f59c1-0ee0-4d9b-b212-2e8dae6ff057-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 90, 'b': 56}, 'id': 'call_LxVltuhGGxtdualNx9CydD2I', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 30, 'b': 12}, 'id': 'call_eiOdqgzGGFmiW2om5T7Xj5Nh', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 322, 'output_tokens': 50, 'total_tokens': 372, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '5040', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'ea77ac7a-e854-4fde-88a6-eac70cdb4dc2', 'tool_call_id': 'call_LxVltuhGGxtdualNx9CydD2I', 'artifact': None, 'status': 'success'}, {'content': '360', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '97570185-077e-4dd2-921d-217464f41a2b', 'tool_call_id': 'call_eiOdqgzGGFmiW2om5T7Xj5Nh', 'artifact': None, 'status': 'success'}, {'content': '', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 18, 'prompt_tokens': 389, 'total_tokens': 407, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUctBB7yA2YYUaAv4AKBwQ6aBl3PU', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--d1975e17-6c8e-4640-a101-a9781321a459-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 5040, 'b': 360}, 'id': 'call_VJQ8PqXeJLLGeM5PZjp58i51', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 389, 'output_tokens': 18, 'total_tokens': 407, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '1814400', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'e534cd4f-3e1e-445f-91a9-bf6b9276a573', 'tool_call_id': 'call_VJQ8PqXeJLLGeM5PZjp58i51', 'artifact': None, 'status': 'success'}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Calculate the factorial of 10', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '3ef250a8-4689-4853-9078-6b5f1734c5e2'}, {'content': \"The factorial of a number n, denoted as n!, is the product of all positive integers less than or equal to n.\\n\\nTo calculate 10!, it would be:\\n\\n10! = 10  9  8  7  6  5  4  3  2  1\\n\\nI'll calculate this by performing the necessary multiplications.\", 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 163, 'prompt_tokens': 134, 'total_tokens': 297, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUct3xLCwVOvNWvrTWDzsTNajlvDv', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--11ff5e1e-c526-4a66-8e09-e0df1c889e07-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 10, 'b': 9}, 'id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 8, 'b': 7}, 'id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 6, 'b': 5}, 'id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 4, 'b': 3}, 'id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 163, 'total_tokens': 297, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '90', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'e2c607a7-dc17-4b87-a8e5-a0aad4e4284c', 'tool_call_id': 'call_l8oWXxwoCrcFF92rSRF5vZYE', 'artifact': None, 'status': 'success'}, {'content': '56', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'a25bde41-d95d-4444-bfb4-86cbe344105a', 'tool_call_id': 'call_CWpqPb1IR7FTiVyQCWCW3S5d', 'artifact': None, 'status': 'success'}, {'content': '30', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '06b10658-dcd9-443f-b2ca-2243b10fd71c', 'tool_call_id': 'call_ACIjJAiwYpzTJCIUrO7RmGDn', 'artifact': None, 'status': 'success'}, {'content': '12', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '086bdada-641a-45cf-9b79-7ee646041082', 'tool_call_id': 'call_DIugAia7RWBCnUpKeCYSaflZ', 'artifact': None, 'status': 'success'}, {'content': '', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 50, 'prompt_tokens': 322, 'total_tokens': 372, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUct8NnPIaDO4Zt5IbDDHul7Xfumg', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--e18f59c1-0ee0-4d9b-b212-2e8dae6ff057-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 90, 'b': 56}, 'id': 'call_LxVltuhGGxtdualNx9CydD2I', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 30, 'b': 12}, 'id': 'call_eiOdqgzGGFmiW2om5T7Xj5Nh', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 322, 'output_tokens': 50, 'total_tokens': 372, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '5040', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'ea77ac7a-e854-4fde-88a6-eac70cdb4dc2', 'tool_call_id': 'call_LxVltuhGGxtdualNx9CydD2I', 'artifact': None, 'status': 'success'}, {'content': '360', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '97570185-077e-4dd2-921d-217464f41a2b', 'tool_call_id': 'call_eiOdqgzGGFmiW2om5T7Xj5Nh', 'artifact': None, 'status': 'success'}, {'content': '', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 18, 'prompt_tokens': 389, 'total_tokens': 407, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUctBB7yA2YYUaAv4AKBwQ6aBl3PU', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--d1975e17-6c8e-4640-a101-a9781321a459-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 5040, 'b': 360}, 'id': 'call_VJQ8PqXeJLLGeM5PZjp58i51', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 389, 'output_tokens': 18, 'total_tokens': 407, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '1814400', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'e534cd4f-3e1e-445f-91a9-bf6b9276a573', 'tool_call_id': 'call_VJQ8PqXeJLLGeM5PZjp58i51', 'artifact': None, 'status': 'success'}, {'content': 'The factorial of 10, denoted as 10!, is 1,814,400.', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 21, 'prompt_tokens': 417, 'total_tokens': 438, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUctCo5aH4eopvEAf0qro24W4XpIZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--30509e18-e2ca-48bb-9b42-0354cb935db4-0', 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 417, 'output_tokens': 21, 'total_tokens': 438, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}]})\n"
     ]
    }
   ],
   "source": [
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "# Input message\n",
    "input_message = HumanMessage(content=\"Calculate the factorial of 10\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
   "metadata": {},
   "source": [
    "The streamed objects have: \n",
    "\n",
    "* `event`: Type\n",
    "* `data`: State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57b735aa-139c-45a3-a850-63519c0004f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "content='Calculate the factorial of 10' additional_kwargs={} response_metadata={} id='51923191-43a2-4303-a66c-4a7cb18e5d39'\n",
      "=========================\n",
      "content='The factorial of a number \\\\( n \\\\) is the product of all positive integers less than or equal to \\\\( n \\\\). It is denoted as \\\\( n! \\\\).\\n\\nFor \\\\( 10! \\\\), it is calculated as:\\n\\n\\\\[ 10! = 10 \\\\times 9 \\\\times 8 \\\\times 7 \\\\times 6 \\\\times 5 \\\\times 4 \\\\times 3 \\\\times 2 \\\\times 1 \\\\]\\n\\nI will calculate this step by step.' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 185, 'total_tokens': 319, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 185, 'prompt_tokens': 134, 'total_tokens': 319, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUcugiD2cousf4M6NozSmqkG3uZTR', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--1e94dd10-2c92-4b57-9d95-bb76bb01d4fe-0' tool_calls=[{'name': 'multiply', 'args': {'a': 10, 'b': 9}, 'id': 'call_3XKFu9eEDjgt5MYb7wXo1spv', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 8, 'b': 7}, 'id': 'call_9QzceKjSqdEnRwuXJhSvkMyS', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 6, 'b': 5}, 'id': 'call_e6b1L17RAFBdxaTRpSW68bb9', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 4, 'b': 3}, 'id': 'call_lS5bIj0g6tqryjDw0YuYluaH', 'type': 'tool_call'}]\n",
      "=========================\n",
      "content='12' name='multiply' id='d769ecd0-f273-4651-a4ce-1fe4b18f89d7' tool_call_id='call_lS5bIj0g6tqryjDw0YuYluaH'\n",
      "=========================\n",
      "content='' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 344, 'output_tokens': 67, 'total_tokens': 411, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 344, 'total_tokens': 411, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUcujgYEd0uDlWU6PuYJprMgR5xgY', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--129e43fa-e93c-4cfb-bc32-92e8d0c2da27-0' tool_calls=[{'name': 'multiply', 'args': {'a': 90, 'b': 56}, 'id': 'call_rYZjgO0hzqLPjusmxQ3XJNEg', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 30, 'b': 12}, 'id': 'call_bUyV8kTU84QLVeGHHhkEhUy3', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 2, 'b': 1}, 'id': 'call_Ybunujzm9M2aoqLZY2UTKvWm', 'type': 'tool_call'}]\n",
      "=========================\n",
      "content='2' name='multiply' id='ee893149-3cab-4927-9687-1310dc617c3a' tool_call_id='call_Ybunujzm9M2aoqLZY2UTKvWm'\n",
      "=========================\n",
      "content='' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 431, 'output_tokens': 18, 'total_tokens': 449, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 431, 'total_tokens': 449, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUculSPvt67XSlTmR9FCY4S6voF8Y', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--8ffe552e-bebc-46c2-98ff-2f6431981de4-0' tool_calls=[{'name': 'multiply', 'args': {'a': 5040, 'b': 360}, 'id': 'call_T7vOJ3Kxn7ktT4rrpHfuQ6KF', 'type': 'tool_call'}]\n",
      "=========================\n",
      "content='1814400' name='multiply' id='557f754e-e521-4605-9494-e2e4ea1200a6' tool_call_id='call_T7vOJ3Kxn7ktT4rrpHfuQ6KF'\n",
      "=========================\n",
      "content='The factorial of 10, denoted as \\\\( 10! \\\\), is 1,814,400.' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 459, 'output_tokens': 24, 'total_tokens': 483, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 459, 'total_tokens': 483, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CUcumeLWNal9rB0U7dEMRdcSiJsn4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--bf517a25-47ea-4b5e-8f24-feea9b39bd42-0'\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Calculate the factorial of 10\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    messages = event.data.get('messages',None)\n",
    "    if messages:\n",
    "        print(convert_to_messages(messages)[-1])\n",
    "    print('='*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555d186-27be-4ddf-934c-895a3105035d",
   "metadata": {},
   "source": [
    "There are some new streaming mode that are only supported via the API.\n",
    "\n",
    "For example, we can [use `messages` mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) to better handle the above case!\n",
    "\n",
    "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
    "\n",
    "All events emitted using `messages` mode have two attributes:\n",
    "\n",
    "* `event`: This is the name of the event\n",
    "* `data`: This is data associated with the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Calculate the factorial of 10\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"messages\"):\n",
    "    print(event.event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
   "metadata": {},
   "source": [
    "We can see a few events: \n",
    "\n",
    "* `metadata`: metadata about the run\n",
    "* `messages/complete`: fully formed message \n",
    "* `messages/partial`: chat model tokens\n",
    "\n",
    "You can dig further into the types [here](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages).\n",
    "\n",
    "Now, let's show how to stream these messages. \n",
    "\n",
    "We'll define a helper function for better formatting of the tool calls in messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Run ID - 019a1c8b-d696-7577-afc6-1ed672192d2c\n",
      "--------------------------------------------------\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10,\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10.\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10!\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! =\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10 \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9 \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8 \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7 \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6 \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5 \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4 \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3 \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2 \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this,\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {'a': 6}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {'a': 6, 'b': 5}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {'a': 6, 'b': 5}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {'a': 6, 'b': 5}\n",
      "Tool Call ID: call_f8Y4TCfiMVNfm5O6b7PiPikj, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {'a': 6, 'b': 5}\n",
      "Tool Call ID: call_f8Y4TCfiMVNfm5O6b7PiPikj, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {'a': 6, 'b': 5}\n",
      "Tool Call ID: call_f8Y4TCfiMVNfm5O6b7PiPikj, Function: multiply, Arguments: {'a': 4}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {'a': 6, 'b': 5}\n",
      "Tool Call ID: call_f8Y4TCfiMVNfm5O6b7PiPikj, Function: multiply, Arguments: {'a': 4, 'b': 3}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {'a': 6, 'b': 5}\n",
      "Tool Call ID: call_f8Y4TCfiMVNfm5O6b7PiPikj, Function: multiply, Arguments: {'a': 4, 'b': 3}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {'a': 6, 'b': 5}\n",
      "Tool Call ID: call_f8Y4TCfiMVNfm5O6b7PiPikj, Function: multiply, Arguments: {'a': 4, 'b': 3}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {'a': 6, 'b': 5}\n",
      "Tool Call ID: call_f8Y4TCfiMVNfm5O6b7PiPikj, Function: multiply, Arguments: {'a': 4, 'b': 3}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "AI: To calculate the factorial of 10, we need to multiply all positive integers up to 10. The calculation is as follows:\n",
      "\n",
      "10! = 10  9  8  7  6  5  4  3  2  1\n",
      "\n",
      "To do this, I will compute the multiplication step-by-step using the functions available.\n",
      "Tool Calls:\n",
      "Tool Call ID: call_daHnCp1jIv9SVA0lf5EeGlvX, Function: multiply, Arguments: {'a': 10, 'b': 9}\n",
      "Tool Call ID: call_uyBJoYCNEwHez7OxCDmzmFts, Function: multiply, Arguments: {'a': 8, 'b': 7}\n",
      "Tool Call ID: call_HJlSCpFcly027dv6XWWqWcSr, Function: multiply, Arguments: {'a': 6, 'b': 5}\n",
      "Tool Call ID: call_f8Y4TCfiMVNfm5O6b7PiPikj, Function: multiply, Arguments: {'a': 4, 'b': 3}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {'a': 90}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {'a': 90}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {'a': 90, 'b': 56}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {'a': 90, 'b': 56}\n",
      "Tool Call ID: call_lHw6qFlJmOgy2Jknf8EWSAUd, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {'a': 90, 'b': 56}\n",
      "Tool Call ID: call_lHw6qFlJmOgy2Jknf8EWSAUd, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {'a': 90, 'b': 56}\n",
      "Tool Call ID: call_lHw6qFlJmOgy2Jknf8EWSAUd, Function: multiply, Arguments: {'a': 30}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {'a': 90, 'b': 56}\n",
      "Tool Call ID: call_lHw6qFlJmOgy2Jknf8EWSAUd, Function: multiply, Arguments: {'a': 30}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {'a': 90, 'b': 56}\n",
      "Tool Call ID: call_lHw6qFlJmOgy2Jknf8EWSAUd, Function: multiply, Arguments: {'a': 30, 'b': 12}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {'a': 90, 'b': 56}\n",
      "Tool Call ID: call_lHw6qFlJmOgy2Jknf8EWSAUd, Function: multiply, Arguments: {'a': 30, 'b': 12}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {'a': 90, 'b': 56}\n",
      "Tool Call ID: call_lHw6qFlJmOgy2Jknf8EWSAUd, Function: multiply, Arguments: {'a': 30, 'b': 12}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_uKfAub1eXmjVnTKSsoxesy2m, Function: multiply, Arguments: {'a': 2, 'b': 1}\n",
      "Tool Call ID: call_YQY1zin5mumaiq40zhKFZ0js, Function: multiply, Arguments: {'a': 90, 'b': 56}\n",
      "Tool Call ID: call_lHw6qFlJmOgy2Jknf8EWSAUd, Function: multiply, Arguments: {'a': 30, 'b': 12}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {'a': 504}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {'a': 5040}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {'a': 5040}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {'a': 5040}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {'a': 5040}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {'a': 5040, 'b': 2}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {'a': 5040, 'b': 2}\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {'a': 5040, 'b': 2}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {'a': 5040, 'b': 2}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_WopcrgEdfvLacYc2Lr5QldgW, Function: multiply, Arguments: {'a': 5040, 'b': 2}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10!)\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10!) is\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10!) is \n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10!) is 3\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10!) is 3,\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10!) is 3,628\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10!) is 3,628,\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10!) is 3,628,800\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10!) is 3,628,800.\n",
      "Response Metadata: Finish Reason - N/A\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10!) is 3,628,800.\n",
      "Response Metadata: Finish Reason - stop\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10!) is 3,628,800.\n",
      "Response Metadata: Finish Reason - stop\n",
      "--------------------------------------------------\n",
      "AI: The factorial of 10 (10!) is 3,628,800.\n",
      "Response Metadata: Finish Reason - stop\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Calculate the factorial of 10\")\n",
    "\n",
    "def format_tool_calls(tool_calls):\n",
    "    \"\"\"\n",
    "    Format a list of tool calls into a readable string.\n",
    "\n",
    "    Args:\n",
    "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
    "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tool_calls:\n",
    "        formatted_calls = []\n",
    "        for call in tool_calls:\n",
    "            formatted_calls.append(\n",
    "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
    "            )\n",
    "        return \"\\n\".join(formatted_calls)\n",
    "    return \"No tool calls\"\n",
    "\n",
    "async for event in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input={\"messages\": [input_message]},\n",
    "    stream_mode=\"messages\",):\n",
    "    \n",
    "    # Handle metadata events\n",
    "    if event.event == \"metadata\":\n",
    "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Handle partial message events\n",
    "    elif event.event == \"messages/partial\":\n",
    "        for data_item in event.data:\n",
    "            # Process user messages\n",
    "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
    "                print(f\"Human: {data_item['content']}\")\n",
    "            else:\n",
    "                # Extract relevant data from the event\n",
    "                tool_calls = data_item.get(\"tool_calls\", [])\n",
    "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
    "                content = data_item.get(\"content\", \"\")\n",
    "                response_metadata = data_item.get(\"response_metadata\", {})\n",
    "\n",
    "                if content:\n",
    "                    print(f\"AI: {content}\")\n",
    "\n",
    "                if tool_calls:\n",
    "                    print(\"Tool Calls:\")\n",
    "                    print(format_tool_calls(tool_calls))\n",
    "\n",
    "                if invalid_tool_calls:\n",
    "                    print(\"Invalid Tool Calls:\")\n",
    "                    print(format_tool_calls(invalid_tool_calls))\n",
    "\n",
    "                if response_metadata:\n",
    "                    finish_reason = response_metadata.get(\"finish_reason\", \"N/A\")\n",
    "                    print(f\"Response Metadata: Finish Reason - {finish_reason}\")\n",
    "                    \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
